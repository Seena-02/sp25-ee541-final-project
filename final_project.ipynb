{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e689da18-ab5c-4a8f-8764-fc073c1d2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.multiprocessing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet34\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report \n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from collections import OrderedDict\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1e2b1a-6e45-4d1d-bdc3-e5afa558f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(479, 6)\n",
      "(480, 6)\n"
     ]
    }
   ],
   "source": [
    "csv_files = glob.glob('Data'+'/**/*.csv', recursive=True)\n",
    "\n",
    "features = []\n",
    "    \n",
    "for file in csv_files:\n",
    "    data = pd.read_csv(file, header=None, na_values=\"?\", skiprows=5)\n",
    "    data.columns = [\"time\", \"avg_rss12\", \"var_rss12\", \"avg_rss13\", \"var_rss13\", \"avg_rss23\", \"var_rss23\"] \n",
    "    data = data.drop(columns = [\"time\"])    \n",
    "    features.append(data)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "for i,f in enumerate(features):\n",
    "    if len(f) != 480:\n",
    "        print(np.shape(f))\n",
    "        for i in range(480-len(f)):\n",
    "            f = pd.concat([f, f.mean().to_frame().T])\n",
    "        print(np.shape(f))\n",
    "    if 'dataset1.csv' in csv_files[i] or 'dataset2.csv' in csv_files[i] or 'dataset3.csv' in csv_files[i]:\n",
    "        if 'dataset3' in csv_files[i] and 'bending' in csv_files[i]:\n",
    "            X_train.append(f)\n",
    "        else:\n",
    "            X_test.append(f)\n",
    "    else:\n",
    "        X_train.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72c2293-8ccb-4a26-9eb7-5a473e32f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "Y_test = []\n",
    "for i,a in enumerate(csv_files):\n",
    "    if 'dataset1.csv' in a or 'dataset2.csv' in a or 'dataset3.csv' in a:\n",
    "        if 'dataset3' in a and 'bending' in a:\n",
    "            Y_train.append(a.split('/')[1])\n",
    "        else:\n",
    "            Y_test.append(a.split('/')[1])\n",
    "    else:\n",
    "        Y_train.append(a.split('/')[1])\n",
    "\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "label_binarizer = LabelBinarizer().fit(Y_train)\n",
    "Y_onehot_test = label_binarizer.transform(Y_test)\n",
    "Y_onehot_train = label_binarizer.transform(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295afa60-8385-4356-9dfd-82c4f19f98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "input_size = 6\n",
    "hidden_size = 42\n",
    "output_size = 7\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "model.to(mps_device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-04, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3040aec-3a16-42e1-86d3-a67abb87f1c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(Y_onehot_train[i], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(mps_device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss_func(output, label)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mSimpleRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      8\u001b[0m     h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), hidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 9\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, h0)\n\u001b[1;32m     10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:684\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 684\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m             )\n\u001b[1;32m    687\u001b[0m         hx \u001b[38;5;241m=\u001b[39m hx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs = [], []\n",
    "\n",
    "model.eval()\n",
    "for epoch in range(40):    \n",
    "    for i in range(len(X_train)):\n",
    "        input = torch.tensor(X_train[i].values, dtype=torch.float32).to(mps_device)\n",
    "        y = torch.tensor(Y_onehot_train[i], dtype=torch.float32).to(mps_device)\n",
    "\n",
    "        # Forward Pass\n",
    "        output = model(input)\n",
    "        y_pred = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_loss = loss_func(output, label).item()\n",
    "        train_acc = 100 * (y_pred == label).sum().item() / len(label)\n",
    "\n",
    "        break\n",
    "        \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673b8fe-dfc2-4056-8166-aabf77e2f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9a064-f58b-46aa-88ea-82c3077c0d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
